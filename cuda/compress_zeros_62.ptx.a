//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33191640
// Cuda compilation tools, release 12.2, V12.2.140
// Based on NVVM 7.0.1
//

.version 8.2
.target sm_62
.address_size 64

	// .globl	compressRLESingleKernel
.extern .shared .align 16 .b8 s_data[];

.visible .entry compressRLESingleKernel(
	.param .u64 compressRLESingleKernel_param_0,
	.param .u64 compressRLESingleKernel_param_1,
	.param .u64 compressRLESingleKernel_param_2,
	.param .u64 compressRLESingleKernel_param_3
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<11>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<48>;
	.reg .b64 	%rd<39>;


	ld.param.u64 	%rd9, [compressRLESingleKernel_param_0];
	ld.param.u64 	%rd10, [compressRLESingleKernel_param_1];
	ld.param.u64 	%rd7, [compressRLESingleKernel_param_2];
	ld.param.u64 	%rd8, [compressRLESingleKernel_param_3];
	cvta.to.global.u64 	%rd1, %rd9;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r14, %r13, %r1, %r2;
	cvt.u64.u32 	%rd2, %r14;
	setp.ge.u64 	%p1, %rd2, %rd7;
	cvta.to.global.u64 	%rd11, %rd10;
	mul.wide.u32 	%rd12, %r14, 4;
	add.s64 	%rd3, %rd11, %rd12;
	mov.u16 	%rs10, 0;
	@%p1 bra 	$L__BB0_3;

	cvt.u32.u64 	%r15, %rd2;
	ld.global.nc.f32 	%f2, [%rd3];
	setp.neu.f32 	%p2, %f2, 0f00000000;
	setp.eq.s32 	%p3, %r15, 0;
	or.pred  	%p4, %p3, %p2;
	mov.u16 	%rs10, 1;
	@%p4 bra 	$L__BB0_3;

	ld.global.nc.f32 	%f3, [%rd3+-4];
	setp.neu.f32 	%p5, %f3, 0f00000000;
	selp.u16 	%rs10, 1, 0, %p5;

$L__BB0_3:
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, s_data;
	add.s32 	%r3, %r17, %r16;
	cvt.u32.u16 	%r4, %rs10;
	st.shared.u32 	[%r3], %r4;
	bar.sync 	0;
	setp.lt.u32 	%p6, %r1, 2;
	@%p6 bra 	$L__BB0_8;

	mov.u32 	%r45, 1;

$L__BB0_5:
	bar.sync 	0;
	setp.lt.u32 	%p7, %r2, %r45;
	@%p7 bra 	$L__BB0_7;

	sub.s32 	%r19, %r2, %r45;
	shl.b32 	%r20, %r19, 2;
	add.s32 	%r22, %r17, %r20;
	ld.shared.u32 	%r23, [%r3];
	ld.shared.u32 	%r24, [%r22];
	add.s32 	%r25, %r23, %r24;
	st.shared.u32 	[%r3], %r25;

$L__BB0_7:
	bar.sync 	0;
	shl.b32 	%r45, %r45, 1;
	setp.lt.u32 	%p8, %r45, %r1;
	@%p8 bra 	$L__BB0_5;

$L__BB0_8:
	ld.shared.u32 	%r26, [%r3];
	ld.shared.u32 	%r27, [%r3+-4];
	mul.lo.s32 	%r28, %r27, %r4;
	sub.s32 	%r7, %r26, %r28;
	setp.eq.s16 	%p9, %rs10, 0;
	@%p9 bra 	$L__BB0_18;

	ld.global.nc.f32 	%f1, [%rd3];
	setp.eq.f32 	%p10, %f1, 0f00000000;
	mul.lo.s32 	%r8, %r7, 5;
	cvt.u64.u32 	%rd13, %r8;
	add.s64 	%rd4, %rd1, %rd13;
	@%p10 bra 	$L__BB0_11;
	bra.uni 	$L__BB0_10;

$L__BB0_11:
	mov.u32 	%r46, 1;

$L__BB0_12:
	cvt.u64.u32 	%rd5, %r46;
	add.s64 	%rd22, %rd5, %rd2;
	setp.ge.u64 	%p11, %rd22, %rd7;
	mov.u32 	%r47, %r46;
	@%p11 bra 	$L__BB0_17;

	shl.b64 	%rd23, %rd5, 2;
	add.s64 	%rd24, %rd3, %rd23;
	ld.global.nc.f32 	%f4, [%rd24];
	setp.neu.f32 	%p12, %f4, 0f00000000;
	mov.u32 	%r47, %r46;
	@%p12 bra 	$L__BB0_17;

	add.s32 	%r47, %r46, 1;
	cvt.u64.u32 	%rd6, %r47;
	add.s64 	%rd25, %rd6, %rd2;
	setp.ge.u64 	%p13, %rd25, %rd7;
	@%p13 bra 	$L__BB0_17;

	shl.b64 	%rd26, %rd6, 2;
	add.s64 	%rd27, %rd3, %rd26;
	ld.global.nc.f32 	%f5, [%rd27];
	setp.neu.f32 	%p14, %f5, 0f00000000;
	@%p14 bra 	$L__BB0_17;

	add.s32 	%r46, %r46, 2;
	setp.ne.s32 	%p15, %r46, -1;
	mov.u32 	%r47, -1;
	@%p15 bra 	$L__BB0_12;

$L__BB0_17:
	mov.u16 	%rs9, 0;
	st.global.u8 	[%rd4], %rs9;
	add.s32 	%r37, %r8, 1;
	cvt.u64.u32 	%rd28, %r37;
	add.s64 	%rd29, %rd1, %rd28;
	st.global.u8 	[%rd29], %r47;
	shr.u32 	%r38, %r47, 8;
	add.s32 	%r39, %r8, 2;
	cvt.u64.u32 	%rd30, %r39;
	add.s64 	%rd31, %rd1, %rd30;
	st.global.u8 	[%rd31], %r38;
	shr.u32 	%r40, %r47, 16;
	add.s32 	%r41, %r8, 3;
	cvt.u64.u32 	%rd32, %r41;
	add.s64 	%rd33, %rd1, %rd32;
	st.global.u8 	[%rd33], %r40;
	shr.u32 	%r42, %r47, 24;
	add.s32 	%r43, %r8, 4;
	cvt.u64.u32 	%rd34, %r43;
	add.s64 	%rd35, %rd1, %rd34;
	st.global.u8 	[%rd35], %r42;
	bra.uni 	$L__BB0_18;

$L__BB0_10:
	mov.b32 	%r29, %f1;
	mov.u16 	%rs5, 255;
	st.global.u8 	[%rd4], %rs5;
	add.s32 	%r31, %r8, 1;
	cvt.u64.u32 	%rd14, %r31;
	add.s64 	%rd15, %rd1, %rd14;
	st.global.u8 	[%rd15], %r29;
	ld.global.nc.u8 	%rs6, [%rd3+1];
	add.s32 	%r32, %r8, 2;
	cvt.u64.u32 	%rd16, %r32;
	add.s64 	%rd17, %rd1, %rd16;
	st.global.u8 	[%rd17], %rs6;
	ld.global.nc.u8 	%rs7, [%rd3+2];
	add.s32 	%r33, %r8, 3;
	cvt.u64.u32 	%rd18, %r33;
	add.s64 	%rd19, %rd1, %rd18;
	st.global.u8 	[%rd19], %rs7;
	ld.global.nc.u8 	%rs8, [%rd3+3];
	add.s32 	%r34, %r8, 4;
	cvt.u64.u32 	%rd20, %r34;
	add.s64 	%rd21, %rd1, %rd20;
	st.global.u8 	[%rd21], %rs8;

$L__BB0_18:
	setp.ne.s32 	%p17, %r2, 0;
	or.pred  	%p18, %p17, %p9;
	@%p18 bra 	$L__BB0_20;

	mad.lo.s32 	%r44, %r7, 5, 5;
	cvt.u64.u32 	%rd36, %r44;
	cvta.to.global.u64 	%rd37, %rd8;
	red.global.add.u64 	[%rd37], %rd36;

$L__BB0_20:
	ret;

}

